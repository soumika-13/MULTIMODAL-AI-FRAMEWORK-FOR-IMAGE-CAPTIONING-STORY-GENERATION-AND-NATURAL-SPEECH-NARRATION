# MULTIMODALAI-FRAMEWORK-FOR-IMAGE-CAPTIONING-STORY-GENERATION-AND-NATURAL-SPEECH-NARRATION

This project uses deep learning models to generate captions and stories from images, followed by text-to-speech conversion. It combines computer vision and natural language processing to bring images to life through storytelling.

---

## ğŸ“Œ Features

- ğŸ” **Image Feature Extraction** using InceptionV3
- ğŸ§  **Caption Generation** using a trained LSTM model
- ğŸ“– **Story Generation** using GPT-2
- ğŸ”Š **Text-to-Speech (TTS)** using Google Text-to-Speech (gTTS)
- ğŸ–¼ï¸ğŸ’¬ **Interactive Interface** built with Streamlit

---

## ğŸ§° Tech Stack

- **Python 3**
- **TensorFlow / Keras**
- **Transformers (Hugging Face GPT-2)**
- **gTTS**
- **Streamlit**
- **Flickr8k Dataset** for training
- **NumPy, Matplotlib, Pickle** and other essentials

---

## ğŸš€ How It Works

1. **Image Upload:** The user uploads an image via the Streamlit interface.
2. **Image Feature Extraction:** InceptionV3 extracts a 2048-dimensional vector from the image.
3. **Caption Generation:** A trained LSTM model predicts a suitable caption.
4. **Story Generation:** GPT-2 generates a story based on the caption.
5. **Text-to-Speech:** gTTS converts the story into audio.
6. **Final Output:** The image, caption, story, and audio are presented to the user.

---

## ğŸ“ Project Structure

â”œâ”€â”€ app.py # Streamlit interface
â”œâ”€â”€ image_captioning.py # LSTM-based caption generation
â”œâ”€â”€ story_generator.py # GPT-2 story generation
â”œâ”€â”€ tts.py # Text-to-Speech with gTTS
â”œâ”€â”€ feature_extractor.py # InceptionV3 model for image features
â”œâ”€â”€ utils.py # Helper functions
â”œâ”€â”€ model/ # Trained models and tokenizers
â”œâ”€â”€ data/ # Sample images and dataset (Flickr8k)
â””â”€â”€ README.md


---


## Setup

1. Clone the repository. - ## Setup

1. Clone the repository. - git clone https://github.com/SrivathsaTirumala/EDUQUEST-AI-AUTOMATED-QUESTION-PAPER-GENERATION-SYSTEM.git
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
3. Run the file:
    python3 run.py

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
3. Run the file:
    python3 run.py


## ğŸ§ª Demo

> ğŸ“Œ *Upload an image â†’ Get a caption and a story â†’ Listen to the story come alive!*  
*(You can include Streamlit app screenshots or video links here)*

---

## ğŸ“š Future Scope

- âœ¨ Use CLIP or BLIP-2 for better captioning
- ğŸ—£ï¸ Improve story coherence using larger LLMs like GPT-3 or GPT-4
- ğŸŒ Deploy as a web app using Hugging Face Spaces or Streamlit Cloud
- ğŸ§  Support multilingual storytelling and emotion-based narration

---


## ğŸŒŸ Acknowledgements

- [Flickr8k Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k)
- [Hugging Face Transformers](https://huggingface.co/)
- [Google Text-to-Speech](https://pypi.org/project/gTTS/)
- [Streamlit](https://streamlit.io/)


## Contact ğŸ“§
- **Gundeboyena Priyanka**
- **Email**: priyanka211216@gmail.com
- **LinkedIn**: https://www.linkedin.com/in/gundeboyena-priyanka-77235a252
- **GitHub** : https://github.com/priyanka9959